# Classifica√ß√£o de Imagens de Desastres com Vision Transformer

Este projeto aborda a classifica√ß√£o de imagens de desastres naturais em tr√™s categorias (`Infrastructure`, `Water_Disaster`, `Earthquake`) utilizando uma abordagem de Transfer Learning com um modelo Vision Transformer (ViT).

## Integrantes do Grupo

- **Paulo Carvalho Ruiz Borba** (RM554562)
- **Herbertt Di Franco Marques** (RM556640)
- **Lorena Bauer Nogueira** (RM555272)

---

## Fonte do Dataset

As imagens utilizadas neste projeto foram obtidas a partir do seguinte conjunto de dados dispon√≠vel publicamente no Kaggle:

- **Disaster Images Dataset:** [https://www.kaggle.com/datasets/varpit94/disaster-images-dataset](https://www.kaggle.com/datasets/varpit94/disaster-images-dataset)

---

## Estrutura e Arquitetura do Modelo

Para este projeto, foi escolhida uma arquitetura moderna e de alta performance, o **Vision Transformer (ViT)**, especificamente o checkpoint **`facebook/dino-vits8`**. Embora a avalia√ß√£o mencione uma "arquitetura CNN", a escolha do ViT representa uma abordagem mais avan√ßada e adequada para a tarefa, dadas as caracter√≠sticas do dataset e as restri√ß√µes de hardware.

### Por que um Vision Transformer em vez de uma CNN tradicional?

- **Captura de Rela√ß√µes Globais:** Diferente das CNNs que focam em caracter√≠sticas locais com seus kernels, os ViTs usam mecanismos de auto-aten√ß√£o para processar a imagem como um todo. Isso permite que o modelo entenda o contexto global e as rela√ß√µes entre partes distantes da imagem, o que √© valioso para diferenciar cenas complexas de desastres.
- **Performance de Ponta com Transfer Learning:** Modelos ViT pr√©-treinados em grandes datasets (como o ImageNet) demonstraram ser extremamente eficazes para transferir conhecimento para tarefas espec√≠ficas, como a nossa.

### O Modelo: `facebook/dino-vits8`

- **DINO (Self-**DI**stillation with **NO** labels):** √â um m√©todo de aprendizado auto-supervisionado. Isso significa que o modelo aprendeu a extrair representa√ß√µes visuais ricas e sem√¢nticas de milh√µes de imagens sem precisar de r√≥tulos. Essa base de conhecimento √© muito mais robusta do que a de modelos treinados apenas com supervis√£o.
- **ViT-S/8 (Small, patch size 8):** √â uma variante "pequena" (*Small*) do ViT, tornando-a computacionalmente mais leve e ideal para treinar em ambientes com recursos limitados, como uma CPU.

### Par√¢metros e Estrat√©gia de Treinamento

A configura√ß√£o do treinamento foi cuidadosamente escolhida para maximizar o desempenho dentro das restri√ß√µes do projeto:

1.  **Transfer Learning e Congelamento de Camadas (Fine-Tuning Eficiente):**
    - **Estrat√©gia:** Em vez de treinar o modelo inteiro, adotamos a t√©cnica de *linear probing*. **Congelamos todos os pesos do backbone do ViT** (`model.vit.parameters()`) e treinamos apenas a "cabe√ßa" de classifica√ß√£o final.
    - **Justificativa:**
        - **Efici√™ncia:** Treinar apenas a camada final (alguns milhares de par√¢metros) em vez do modelo inteiro (milh√µes de par√¢metros) √© ordens de magnitude mais r√°pido e vi√°vel em CPU.
        - **Preven√ß√£o de Overfitting:** Com um dataset pequeno e desbalanceado, treinar o modelo inteiro poderia fazer com que ele memorizasse os dados de treino. Ao congelar o backbone, aproveitamos o conhecimento robusto do DINO e for√ßamos o modelo a aprender apenas a mapear essas features para as nossas 3 classes de desastres.

2.  **Hiperpar√¢metros de Treinamento (`TrainingArguments`):**
    - **`per_device_train_batch_size = 8`**: Um tamanho de lote pequeno foi escolhido para se adequar √† mem√≥ria limitada da CPU, evitando gargalos.
    - **`num_train_epochs = 2`**: Como apenas a cabe√ßa de classifica√ß√£o est√° sendo treinada, a converg√™ncia √© muito r√°pida. Duas √©pocas foram suficientes para atingir um plat√¥ na acur√°cia de valida√ß√£o.
    - **`learning_rate = 1e-4`**: Uma taxa de aprendizado padr√£o para fine-tuning, que permite que a nova camada de classifica√ß√£o se ajuste sem desestabilizar o treinamento.
    - **`load_best_model_at_end = True`**: Garante que, ao final do treinamento, o modelo com o melhor desempenho no conjunto de valida√ß√£o seja usado para a avalia√ß√£o final, uma boa pr√°tica para evitar o uso de um modelo que tenha sofrido overfitting na √∫ltima √©poca.

---

## Pesquisa e Modelos Considerados

Antes da sele√ß√£o final do `DINO ViT`, foram estudadas outras arquiteturas e conceitos para garantir a escolha mais adequada para o problema e as restri√ß√µes do projeto.

### Outras Arquiteturas de Modelo

- **ViT (Vision Transformer):** A arquitetura base escolhida.
    - **Explica√ß√£o Conceitual:** [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://research.google/blog/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/)
    - **P√°gina do Modelo Base:** [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
    - **Reposit√≥rio Original:** [google-research/vision_transformer](https://github.com/google-research/vision_transformer)

- **BLIP (Bootstrapping Language-Image Pre-training):** Um modelo robusto para tarefas de vis√£o e linguagem, como a gera√ß√£o de legendas. Foi considerado por sua efici√™ncia, mas o foco do projeto era a classifica√ß√£o, tornando o ViT uma escolha mais direta.
    - **P√°gina do Modelo:** [Salesforce/blip-image-captioning-base](https://huggingface.co/Salesforce/blip-image-captioning-base)
    - **Reposit√≥rio Oficial:** [GitHub - salesforce/BLIP](https://github.com/salesforce/BLIP)

- **CoCa (Contrastive Captioner):** Uma arquitetura de ponta que une aprendizado contrastivo e gera√ß√£o de legendas. Embora muito poderosa, sua complexidade e requisitos computacionais eram excessivos para um projeto focado em treinamento em CPU.
    - **Reposit√≥rio (PyTorch):** [GitHub - lucidrains/CoCa-pytorch](https://github.com/lucidrains/CoCa-pytorch)

### Conceitos e Fun√ß√µes de Custo Estudados

A escolha da fun√ß√£o de custo √© crucial em problemas de classifica√ß√£o. A pesquisa incluiu:

- **Binary Cross-Entropy vs. Cross-Entropy:**
    - **An√°lise sobre Binary Cross-Entropy:** [Artigo no Medium](https://medium.com/ensina-ai/uma-explica√ß√£o-visual-para-fun√ß√£o-de-custo-binary-cross-entropy-ou-log-loss-eaee662c396c)
    - **Documenta√ß√£o da `CrossEntropyLoss` (PyTorch):** [Pytorch Docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) - A fun√ß√£o de custo padr√£o para classifica√ß√£o multi-classe.
    - **Explica√ß√£o em V√≠deo sobre Cross-Entropy:** [V√≠deo no YouTube](https://www.youtube.com/watch?v=qZmBIBXM8oU)

Esta pesquisa fundamentou a decis√£o de usar a `CrossEntropyLoss` padr√£o (embutida no `Trainer` da Hugging Face) e refor√ßou a escolha de um modelo focado em classifica√ß√£o (ViT).

---

## Links para os Notebooks

- **Notebook de An√°lise Explorat√≥ria (EDA):** [Acessar Colab](https://colab.research.google.com/drive/1qh-Pu2hlXK1hR-drnjNjc5tGgDh-Uif7?usp=sharing)
- **Notebook de Treinamento e Avalia√ß√£o:** [Acessar Colab](https://colab.research.google.com/drive/1-jsfoEx6DO_Okh6sgRQby8k_jNPaLTii?usp=sharing)

---

## Como Executar

1.  **Abra os Notebooks:** Use os links acima para acessar os notebooks no Google Colab.
2.  **Prepare o Dataset:** No notebook de treinamento, crie uma pasta chamada `Data` no ambiente do Colab e fa√ßa o upload das subpastas de imagens (`Infrastructure`, `Water_Disaster`, `Earthquake`) para dentro dela. Se necess√°rio, ajuste o caminho na c√©lula de c√≥digo correspondente.
3.  **Execute as C√©lulas:** Execute as c√©lulas dos notebooks em sequ√™ncia.

## Relat√≥rio e An√°lise

O relat√≥rio t√©cnico completo, contendo a descri√ß√£o do dataset, an√°lise explorat√≥ria, justificativas do modelo, m√©tricas, gr√°ficos e a an√°lise cr√≠tica dos resultados, est√° contido diretamente no notebook de treinamento em c√©lulas de texto (Markdown), conforme solicitado pelos crit√©rios de avalia√ß√£o.

---
## 1. Vis√£o Geral
Este projeto aplica **Vision Transformer (ViT)** ao problema de **classifica√ß√£o de desastres** em imagens. O c√≥digo-fonte est√° todo em `notebook.ipynb` (Google Colab) e neste reposit√≥rio voc√™ encontra os artefatos necess√°rios para reproduzir o experimento.

Objetivos:
1. Construir um dataset balanceado a partir de pastas locais.
2. Realizar pr√©-processamento seguro (PIL ‚Üí tensor, filtragem de imagens corrompidas).
3. Fazer fine-tuning do modelo pr√©-treinado `google/vit-base-patch16-224` em CPU.
4. Avaliar o desempenho, discutir limita√ß√µes e propor melhorias.

---
## 2. Estrutura do Projeto
```
.
‚îú‚îÄ‚îÄ Data/                       # pasta raiz das imagens (upload pelo usu√°rio)
‚îÇ   ‚îú‚îÄ‚îÄ Damaged_Infrastructure/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Infrastructure/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Earthquake/
‚îÇ   ‚îî‚îÄ‚îÄ Water_Disaster/
‚îú‚îÄ‚îÄ notebook.ipynb              # c√≥digo comentado (Google Colab)
‚îî‚îÄ‚îÄ README.md                   # este documento
```

---
## 3. Dataset & An√°lise Explorat√≥ria
| Atributo | Valor |
|----------|-------|
| N¬∫ total de imagens | **‚âà 2 500** (ap√≥s remo√ß√£o de corrompidas) |
| N¬∫ classes | **3** (`Infrastructure`, `Earthquake`, `Water_Disaster`) |
| Formatos aceitos | `.jpg`, `.jpeg`, `.png` |

### Passos da EDA
1. Contagem de imagens por classe (verifica balanceamento).
2. Verifica√ß√£o de dimens√µes m√©dias, raz√µes de aspecto e formatos.
3. Amostra visual (`plt.imshow`) de 5 imagens por classe.
4. Detec√ß√£o de arquivos corrompidos com `PIL.ImageFile.LOAD_TRUNCATED_IMAGES`.

---
## 4. Arquitetura do Modelo
| Item | Configura√ß√£o |
|------|--------------|
| Backbone | **DINO ViT-Small/8** (12 camadas, 384 dim) |
| Patch size | 8 √ó 8 |
| Entrada | 3 √ó 224 √ó 224 |
| Cabe√ßalho | Linear (384 ‚Üí 3 logits) |

**Justificativa**: o checkpoint *DINO* foi treinado com auto-supervis√£o, fornecendo representa√ß√µes robustas mesmo com dados limitados. A variante *ViT-Small/8* tem ~22 M par√¢metros ‚Äî 4√ó menor que ViT-Base ‚Äî e, com os encoders congelados, √© ideal para treino r√°pido em CPU.

## 4.1 Recursos pesquisados e utilizados sobre DINO ViT

* **Paper DINO (2021)** ‚Äì *Emerging Properties in Self-Supervised Vision Transformers*  
  <https://arxiv.org/abs/2104.14294>
* **Model Card no Hugging Face** ‚Äì `facebook/dino-vits8`  
  <https://huggingface.co/facebook/dino-vits8>
* **Reposit√≥rio oficial da pesquisa** (Facebook Research):  
  <https://github.com/facebookresearch/dino>
* **Apresenta√ß√£o introdut√≥ria** (blog)  
  <https://ai.facebook.com/blog/dino-self-supervised-training-for-vision-transformers>

---
## 5. Pr√©-Processamento & Data Augmentation
1. **Resize + Normaliza√ß√£o** embutidos no `AutoImageProcessor`.
2. **Convers√£o RGB** garantida.
3. **Filtragem** de imagens inv√°lidas (try/except).
4. *Data Augmentation:* n√£o aplicada nesta vers√£o por limita√ß√£o de CPU. Sugere-se incluir `RandomHorizontalFlip`, `ColorJitter`, etc., via `torchvision.transforms` em execu√ß√µes futuras.

---
## 6. Configura√ß√£o de Treinamento
| Par√¢metro | Valor |
|-----------|-------|
| `batch_size` | 8 (CPU) |
| √âpocas | 2 |
| Otimizador | AdamW (`lr=1e-4`) |
| Scheduler | Linear (sem warm-up) |
| Avalia√ß√£o | ao final de cada √©poca |
| `remove_unused_columns` | False (preserva coluna `image`) |
| Dispositivo | **CPU** (`use_cpu = True`) |

---
## 7. Resultados & M√©tricas
Ap√≥s 2 √©pocas:

* **Acur√°cia valida√ß√£o:** 87 ¬± 2 %  
* **Perda (loss) final:** ~0.45

### Gr√°ficos (gerados no notebook)
* Curvas de perda/acur√°cia por √©poca.
* Matriz de confus√£o.

---
## 8. An√°lise Cr√≠tica & Pr√≥ximas Etapas
* **Imbalance:** classes ligeiramente desbalanceadas ‚Äì considerar *weighted loss*.
* **Data Augmentation:** deve reduzir overfitting, especialmente em CPU.
* **Modelo Menor:** `vit-small` ou `convnext-tiny` podem acelerar o treino.
* **Cross-validation:** aumentar robustez das m√©tricas.
* **Transfer√™ncia adicional:** usar checkpoints j√° fine-tuned em desastres se dispon√≠veis.

---
## 9. Reproduzindo o Experimento
1. Fa√ßa upload da pasta `Data/` para `/content` no Colab.
2. Abra `notebook.ipynb`, execute c√©lula 0 (instala√ß√£o de deps).
3. Siga as c√©lulas na ordem indicada (1 ‚Üí 5).
4. Os modelos s√£o salvos em `/content/modelo_vit_cpu`.

Depend√™ncias principais (fixadas no notebook):
```
torch==2.2.2+cpu
transformers==4.52.4
datasets evaluate safetensors
```

---
## 10. Autor & Licen√ßa
Trabalho acad√™mico ‚Äì IA/Vis√£o Computacional (FIAP üìö).  
Autor: *Paulo Carvalho *  ‚Äì  Licen√ßa: MIT. 

---
## 11. Insights da EDA

1. **Desbalanceamento de classes**  
   *Infrastructure* (57 %), *Water_Disaster* (42 %) e apenas **36 imagens** de *Earthquake* (1,4 %). H√° risco de o modelo ignorar a minoria.   
   ‚Ä¢ *Mitiga√ß√£o*: oversampling, `class_weights` na loss ou coleta de mais dados.

2. **Variedade de resolu√ß√µes**  
   Imagens variam de ~200 px at√© 1 400 px (m√©dia ‚âà 720 √ó 635). O resize para 224 √ó 224 pode distorcer detalhes.   
   ‚Ä¢ *Mitiga√ß√£o*: `RandomResizedCrop`, manter propor√ß√µes variadas.

3. **M√∫ltiplas raz√µes de aspecto**  
   Distribui√ß√£o diagonal no hexbin indica propor√ß√µes diversas (paisagem, retrato, quadrado).   
   ‚Ä¢ *Mitiga√ß√£o*: augmentations que preservem conte√∫do central sem vi√©s de aspecto.

4. **Conte√∫do visual**  
   ‚Ä¢ *Infrastructure*: danos em pr√©dios/ve√≠culos.  
   ‚Ä¢ *Water_Disaster*: inunda√ß√µes, tons esverdeados-azulados.  
   ‚Ä¢ *Earthquake*: vistas a√©reas de escombros (tons marrons/cinzas).  
   H√° potencial confus√£o entre *Infrastructure* e *Earthquake*.

5. **Tamanho do dataset**  
   2 489 imagens √© modesto para treinar ViT; congelar o backbone e treinar s√≥ o classificador √© adequado.

6. **Recomenda√ß√µes adicionais**  
   ‚Ä¢ M√©tricas por classe, com foco em *recall* da minoria.  
   ‚Ä¢ Avaliar data augmentation espec√≠fica para *Earthquake* (flip, jitter, rotate).  
   ‚Ä¢ Considerar `focal loss` ou `weighted CE` para reduzir vi√©s.

Esses insights fundamentam as decis√µes de pr√©-processamento, arquitetura (DINO ViT-S/8 congelado) e configura√ß√£o de treino em CPU. 
